{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb69cf5-bf44-40e1-9dde-ab223128bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib\n",
    "from mmseg.core.evaluation import mean_iou\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from contextlib import redirect_stdout\n",
    "from easydict import EasyDict\n",
    "from models.builder import EncoderDecoder\n",
    "from utils.pyt_utils import load_model\n",
    "from utils.transforms import normalize\n",
    "from dataloader.RGBXDataset import image_decoder\n",
    "\n",
    "trap = io.StringIO()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc0662-e483-4cb7-a2f6-0010e7bfb912",
   "metadata": {},
   "source": [
    "# PanoContext RGB-D Panoramic Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ebd56-c4bb-4753-95ab-7ccc8142ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "config = EasyDict()\n",
    "\n",
    "config.fold = 'F1'\n",
    "config.root = '<sfss_mmsi_path>' # TODO: change this to your own path\n",
    "config.dataset_path = osp.join(config.root, 'datasets', '2D-3D-Semantics-1K')\n",
    "config.dataset_name = 'Stanford2D3DS'\n",
    "config.ignore_index = 255\n",
    "config.image_height = 512\n",
    "config.image_width = 512\n",
    "config.norm_mean = np.array([0.485, 0.456, 0.406])\n",
    "config.norm_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "config.backbone = 'dual_mit_b2' # TODO: change to 'mit_b2', 'dual_mit_b2'\n",
    "config.pretrained_model = osp.join(config.root, 'pretrained', 'segformers/mit_b2.pth')\n",
    "config.decoder = 'DMLPDecoderV2'\n",
    "config.decoder_embed_dim = 512\n",
    "config.optimizer = 'AdamW'\n",
    "config.use_dcns = [True, False, False, False]\n",
    "\n",
    "config.batch_size = 1\n",
    "config.rgb = 'camera-rgb-1K'\n",
    "config.ann = 'camera-semantic-1K'\n",
    "config.modality_x = ['camera-depth-1K']\n",
    "config.train_source = osp.join(config.dataset_path, f'train_{config.fold}.txt')\n",
    "config.eval_source = osp.join(config.dataset_path, f'test_{config.fold}.txt')\n",
    "config.num_classes = 13\n",
    "config.train_scale_array = [0.5, 0.75, 1, 1.25, 1.5, 1.75]\n",
    "if config.backbone == 'mit_b2':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_{config.fold}'))\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-depth-1K':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_Depth_{config.fold}'))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "config.checkpoint_pth = os.path.join(os.path.abspath(os.path.join(config.log_dir, 'checkpoint')), 'epoch-best.pth')\n",
    "config.eval_crop_size = [512, 1024]  # [height weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3a9f8-034e-45f2-a919-8eb8f851b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eval_image_rgbX(image, modal_x1, modal_x2, norm_mean, norm_std):\n",
    "    image = normalize(image, norm_mean, norm_std)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = np.ascontiguousarray(image[None, :, :, :], dtype=np.float32)\n",
    "    image = torch.FloatTensor(image).cuda(device)\n",
    "\n",
    "    modal_x1 = normalize(modal_x1, norm_mean, norm_std)\n",
    "    modal_x1 = modal_x1.transpose(2, 0, 1)\n",
    "    modal_x1 = np.ascontiguousarray(modal_x1[None, :, :, :], dtype=np.float32)\n",
    "    modal_x1 = torch.FloatTensor(modal_x1).cuda(device)\n",
    "\n",
    "    modal_x2 = normalize(modal_x2, norm_mean, norm_std)\n",
    "    modal_x2 = modal_x2.transpose(2, 0, 1)\n",
    "    modal_x2 = np.ascontiguousarray(modal_x2[None, :, :, :], dtype=np.float32)\n",
    "    modal_x2 = torch.FloatTensor(modal_x2).cuda(device)\n",
    "    \n",
    "    return image, modal_x1, modal_x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180fb2e-197b-47df-bb20-607b067751dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_path = os.path.join(config.root, 'figures/pano_asmasuxybohhcj.png')\n",
    "rgb = image_decoder(rgb_path, 'rgb')\n",
    "rgb = np.array(rgb * 255.0, np.uint8) # (H, W, 3)\n",
    "\n",
    "depth_path = os.path.join(config.root, 'figures/pano_asmasuxybohhcj.depth.png')\n",
    "x = image_decoder(depth_path, 'i')\n",
    "x = np.array(x * 255.0, np.uint8)\n",
    "# ignore max depth (65535 -> 0)\n",
    "x = np.where(x == 255, 0, x)\n",
    "# single channel -> 3 channels\n",
    "depth = cv2.merge([x, x, x]) # (H, W, 3)\n",
    "\n",
    "image, modal_x1, modal_x2 = process_eval_image_rgbX(rgb, depth, depth, config.norm_mean, config.norm_std)\n",
    "\n",
    "valid_labels = np.arange(config.num_classes).tolist() + [config.ignore_index]\n",
    "with open(os.path.join(config.dataset_path, 'assets/colors.npy'), 'rb') as f:\n",
    "    seg_colors = np.load(f)\n",
    "with open(os.path.join(config.dataset_path, 'assets/name2label.json'), 'r') as f:\n",
    "    name2id = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42282551-1c45-4bdd-8680-60e9baa76aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input\n",
    "if config.backbone == 'mit_b2':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB']], figsize=(15, 10), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-depth-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth']], figsize=(15, 15), layout='constrained')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "img = image.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "img = (img * 255.0).astype('uint8')\n",
    "plt_img1 = axs['RGB'].imshow(img)\n",
    "axs['RGB'].set_axis_off()\n",
    "axs['RGB'].set_title('RGB Input')\n",
    "\n",
    "if config.backbone != 'mit_b2' and 'camera-depth-1K' in config.modality_x:\n",
    "    dep = modal_x1.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    # dep = (dep * 255.0).astype('uint8')\n",
    "    # dep = np.where(dep[..., 0] == 255, 0.0, dep[..., 0] / 255.0) * 128.0\n",
    "    dep = np.where(dep[..., 0] == 1.0, 0.0, dep[..., 0]) * 128.0\n",
    "    plt_img2 = axs['Depth'].imshow(dep, vmin=0, vmax=10, cmap='jet')\n",
    "    axs['Depth'].set_axis_off()\n",
    "    axs['Depth'].set_title('Depth Input')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d561a8-d4be-428b-bb69-c41fe372aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "network = EncoderDecoder(cfg=config, criterion=None, norm_layer=nn.BatchNorm2d)\n",
    "model = load_model(network, config.checkpoint_pth).to(device)\n",
    "\n",
    "# redirect stdout\n",
    "with redirect_stdout(trap):\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49a58a-76eb-40db-a9bc-144a8c4e9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "assert list(image.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x1.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x2.shape[-2:]) == config.eval_crop_size\n",
    "with torch.no_grad():\n",
    "    if config.backbone == 'mit_b2' or len(config.modality_x) == 1:\n",
    "        score = model.forward(image, modal_x1)\n",
    "    elif len(config.modality_x) == 2:\n",
    "        score = model.forward(image, modal_x1, modal_x2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "output = torch.exp(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b037fe-e9fd-4878-9cb3-72dfbb3809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "\n",
    "fig, axs = plt.subplot_mosaic([['Pred']], figsize=(15, 10), layout='constrained')\n",
    "\n",
    "predict = torch.argmax(output.long(), dim=1) + 1\n",
    "pred = predict.squeeze().cpu().numpy().astype('uint8')\n",
    "unlabeled = np.array(config.ignore_index + 1).astype(np.uint8)\n",
    "if config.backbone == 'mit_b2':\n",
    "    pred[img.sum(-1) == 0] = unlabeled  # mask as unknown id: 0\n",
    "# pred[gt == unlabeled] = unlabeled  # mask as unknown id\n",
    "axs['Pred'].imshow(seg_colors[pred])\n",
    "axs['Pred'].set_axis_off()\n",
    "axs['Pred'].set_title('Semantic Prediction')\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color=seg_colors[seg_val]/255., label=seg_lbl)\n",
    "    for seg_lbl, seg_val in name2id.items()\n",
    "]\n",
    "\n",
    "plt.legend(handles=patches, loc='lower center', ncol=7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa5f3a-1994-448d-b316-d6aeb9a53028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
