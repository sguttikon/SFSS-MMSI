{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22538af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmseg.core.evaluation import mean_iou\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from contextlib import redirect_stdout\n",
    "from easydict import EasyDict\n",
    "from models.builder import EncoderDecoder\n",
    "from utils.pyt_utils import load_model\n",
    "from utils.transforms import normalize\n",
    "from eval import get_matterport3d_pan_pipeline as get_eval_pipeline\n",
    "from dataloader.dataloader import get_matterport3d_pan_pipeline as get_train_pipeline\n",
    "\n",
    "trap = io.StringIO()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef1438",
   "metadata": {},
   "source": [
    "# Matterport3D RGB-D Panoramic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "config = EasyDict()\n",
    "save_figures = False\n",
    "resolution = 50\n",
    "\n",
    "config.root = '<sfss_mmsi_path>' # TODO: change this to your own path\n",
    "config.dataset_path = osp.join(config.root, 'datasets', 'Matterport3D-1K')\n",
    "config.dataset_name = 'Matterport3D'\n",
    "config.ignore_index = 255\n",
    "config.image_height = 512\n",
    "config.image_width = 512\n",
    "config.norm_mean = np.array([0.485, 0.456, 0.406])\n",
    "config.norm_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "config.backbone = 'trio_mit_b2' # TODO: change to 'mit_b2', 'dual_mit_b2', 'trio_mit_b2'\n",
    "config.pretrained_model = osp.join(config.root, 'pretrained', 'segformers/mit_b2.pth')\n",
    "config.decoder = 'DMLPDecoderV2'\n",
    "config.decoder_embed_dim = 512\n",
    "config.optimizer = 'AdamW'\n",
    "config.use_dcns = [True, False, False, False]\n",
    "\n",
    "config.batch_size = 1\n",
    "config.rgb = 'rgb-1K'\n",
    "config.ann = 'semantic-1K'\n",
    "config.modality_x = ['depth-1K'] # TODO: change to 'depth-1K', 'normal-1K'\n",
    "config.train_source = osp.join(config.dataset_path, 'train.txt')\n",
    "config.eval_source = osp.join(config.dataset_path, 'validation.txt')\n",
    "config.test_source = osp.join(config.dataset_path, 'test.txt')\n",
    "config.train_scale_array = [0.5, 0.75, 1, 1.25, 1.5, 1.75]\n",
    "config.num_classes = 41\n",
    "if config.backbone == 'mit_b2':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Matterport3D_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2'))\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'depth-1K':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Matterport3D_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Depth'))\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'normal-1K':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Matterport3D_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Normal'))\n",
    "elif config.backbone == 'trio_mit_b2':\n",
    "    config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Matterport3D_1024x512',\n",
    "                                              'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Depth_Normal'))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "config.checkpoint_pth = os.path.join(os.path.abspath(os.path.join(config.log_dir, 'checkpoint')), 'epoch-best.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f300aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eval_image_rgbX(image, modal_x1, modal_x2, norm_mean, norm_std):\n",
    "    image = normalize(image, norm_mean, norm_std)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = np.ascontiguousarray(image[None, :, :, :], dtype=np.float32)\n",
    "    image = torch.FloatTensor(image).cuda(device)\n",
    "\n",
    "    modal_x1 = normalize(modal_x1, norm_mean, norm_std)\n",
    "    modal_x1 = modal_x1.transpose(2, 0, 1)\n",
    "    modal_x1 = np.ascontiguousarray(modal_x1[None, :, :, :], dtype=np.float32)\n",
    "    modal_x1 = torch.FloatTensor(modal_x1).cuda(device)\n",
    "\n",
    "    modal_x2 = normalize(modal_x2, norm_mean, norm_std)\n",
    "    modal_x2 = modal_x2.transpose(2, 0, 1)\n",
    "    modal_x2 = np.ascontiguousarray(modal_x2[None, :, :, :], dtype=np.float32)\n",
    "    modal_x2 = torch.FloatTensor(modal_x2).cuda(device)\n",
    "    \n",
    "    return image, modal_x1, modal_x2\n",
    "\n",
    "def process_train_image_rgbX(image, modal_x1, modal_x2, norm_mean, norm_std):\n",
    "    image = np.ascontiguousarray(image[None, :, :, :], dtype=np.float32)\n",
    "    image = torch.FloatTensor(image).cuda(device)\n",
    "\n",
    "    modal_x1 = np.ascontiguousarray(modal_x1[None, :, :, :], dtype=np.float32)\n",
    "    modal_x1 = torch.FloatTensor(modal_x1).cuda(device)\n",
    "\n",
    "    modal_x2 = np.ascontiguousarray(modal_x2[None, :, :, :], dtype=np.float32)\n",
    "    modal_x2 = torch.FloatTensor(modal_x2).cuda(device)\n",
    "    \n",
    "    return image, modal_x1, modal_x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e14723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured3d data\n",
    "print('data streaming.....')\n",
    "valid_pipeline = get_eval_pipeline(config, split_name='validation')\n",
    "process_image_rgbX = process_eval_image_rgbX\n",
    "config.eval_crop_size = [512, 1024]  # [height weight]\n",
    "\n",
    "# valid_pipeline = get_train_pipeline(config)\n",
    "# process_image_rgbX = process_train_image_rgbX\n",
    "# config.eval_crop_size = [512, 512]  # [height weight]\n",
    "\n",
    "valid_data_itr = iter(valid_pipeline)\n",
    "valid_labels = np.arange(config.num_classes).tolist() + [config.ignore_index]\n",
    "with open(os.path.join(config.dataset_path, 'assets/colors.npy'), 'rb') as f:\n",
    "    seg_colors = np.load(f)\n",
    "with open(os.path.join(config.dataset_path, 'assets/name2label.json'), 'r') as f:\n",
    "    name2id = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "for _ in range(2):\n",
    "    batch = next(valid_data_itr)\n",
    "image = batch[config.rgb]\n",
    "target = batch[config.ann]\n",
    "modal_x1 = batch[config.modality_x[0]]\n",
    "modal_x2 = batch[config.modality_x[1]] if len(config.modality_x) == 2 else modal_x1\n",
    "filename = batch['sample_token']\n",
    "print(filename)\n",
    "\n",
    "image, modal_x1, modal_x2 = process_image_rgbX(image, modal_x1, modal_x2, config.norm_mean, config.norm_std)\n",
    "target = torch.LongTensor(target.astype('int32')).unsqueeze(0).cuda(device)\n",
    "assert set(torch.unique(target).tolist()).issubset(valid_labels), 'Unknown target label'\n",
    "\n",
    "# visualize input\n",
    "if config.backbone == 'mit_b2':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['GT']], figsize=(15, 12), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'depth-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth'], ['GT']], figsize=(15, 18), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'normal-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['NOR'], ['GT']], figsize=(15, 18), layout='constrained')\n",
    "elif config.backbone == 'trio_mit_b2':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth'], ['NOR'], ['GT']], figsize=(15, 24), layout='constrained')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "img = image.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "img = (img * 255.0).astype('uint8')\n",
    "plt_img1 = axs['RGB'].imshow(img)\n",
    "axs['RGB'].set_axis_off()\n",
    "axs['RGB'].set_title('RGB Input')\n",
    "\n",
    "if config.backbone != 'mit_b2' and 'depth-1K' in config.modality_x:\n",
    "    dep = modal_x1.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    # dep = (dep * 255.0).astype('uint8')\n",
    "    # dep = np.where(dep[..., 0] == 255, 0.0, dep[..., 0] / 255.0) * 16.384\n",
    "    dep = np.where(dep[..., 0] == 1.0, 0.0, dep[..., 0]) * 16.384\n",
    "    plt_img2 = axs['Depth'].imshow(dep, vmin=0, vmax=10, cmap='jet')\n",
    "    axs['Depth'].set_axis_off()\n",
    "    axs['Depth'].set_title('Depth Input')\n",
    "\n",
    "if config.backbone != 'mit_b2' and 'normal-1K' in config.modality_x:\n",
    "    nor = modal_x2.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    nor = (nor * 255.0).astype('uint8')\n",
    "    plt_img3 = axs['NOR'].imshow(nor)\n",
    "    axs['NOR'].set_axis_off()\n",
    "    axs['NOR'].set_title('Normal Input')\n",
    "\n",
    "groundtruth = target.long() + 1\n",
    "gt = groundtruth.squeeze().cpu().numpy().astype('uint8')\n",
    "axs['GT'].imshow(seg_colors[gt])\n",
    "axs['GT'].set_axis_off()\n",
    "axs['GT'].set_title('Semantic GT')\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color=seg_colors[seg_val], label=seg_lbl)\n",
    "    for seg_lbl, seg_val in name2id.items()\n",
    "]\n",
    "\n",
    "plt.legend(handles=patches, loc='lower center', ncol=7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "network = EncoderDecoder(cfg=config, criterion=None, norm_layer=nn.BatchNorm2d)\n",
    "model = load_model(network, config.checkpoint_pth).to(device)\n",
    "\n",
    "# redirect stdout\n",
    "with redirect_stdout(trap):\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ff9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "assert list(image.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x1.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x2.shape[-2:]) == config.eval_crop_size\n",
    "with torch.no_grad():\n",
    "    if config.backbone == 'mit_b2' or len(config.modality_x) == 1:\n",
    "        score = model.forward(image, modal_x1)\n",
    "    elif len(config.modality_x) == 2:\n",
    "        score = model.forward(image, modal_x1, modal_x2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "output = torch.exp(score)\n",
    "iou_result = mean_iou(results=output.argmax(1).cpu().numpy(), gt_seg_maps=target.cpu().numpy(),\n",
    "                      num_classes=config.num_classes, ignore_index=config.ignore_index, nan_to_num=None,\n",
    "                      label_map=dict(), reduce_zero_label=False)\n",
    "\n",
    "id2class = ['void', 'wall', 'floor', 'chair', 'door', 'table', 'picture', 'cabinet', 'cushion', 'window',\n",
    "            'sofa', 'bed', 'curtain', 'chest of drawers', 'plant', 'sink', 'stairs', 'ceiling', 'toilet',\n",
    "            'stool', 'towel', 'mirror', 'tv monitor', 'shower', 'column', 'bathtub', 'counter', 'fireplace',\n",
    "            'lighting', 'beam', 'railing', 'shelving', 'blinds', 'gym equipment', 'seating', 'board panel',\n",
    "            'furniture', 'appliances', 'clothes', 'objects', 'misc']\n",
    "for name, iou, acc in zip(id2class, iou_result['IoU'], iou_result['Acc']):\n",
    "    print(f'{name:20s}:    iou {iou*100:5.3f}    /    acc {acc*100:5.3f}')\n",
    "\n",
    "print('Eval mAcc: {:.3f}, aAcc: {:.3f}, mIoU: {:.3f}'.format(np.nanmean(iou_result['Acc']) * 100,\n",
    "                                                             iou_result['aAcc'] * 100,\n",
    "                                                             np.nanmean(iou_result['IoU']) * 100))\n",
    "miou = round(np.nanmean(iou_result['IoU']) * 100, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    [['GT'], ['Pred']], figsize=(15, 12), layout='constrained')\n",
    "\n",
    "axs['GT'].imshow(seg_colors[gt])\n",
    "axs['GT'].set_axis_off()\n",
    "axs['GT'].set_title('Semantic GT')\n",
    "\n",
    "predict = torch.argmax(output.long(), dim=1) + 1\n",
    "pred = predict.squeeze().cpu().numpy().astype('uint8')\n",
    "unlabeled = np.array(config.ignore_index + 1).astype(np.uint8)\n",
    "if True:\n",
    "    pred[img.sum(-1) == 0] = unlabeled  # mask as unknown id: 0\n",
    "# pred[gt == unlabeled] = unlabeled  # mask as unknown id\n",
    "axs['Pred'].imshow(seg_colors[pred])\n",
    "axs['Pred'].set_axis_off()\n",
    "axs['Pred'].set_title('Semantic Prediction')\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color=seg_colors[seg_val], label=seg_lbl)\n",
    "    for seg_lbl, seg_val in name2id.items()\n",
    "]\n",
    "\n",
    "plt.legend(handles=patches, loc='lower center', ncol=7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_figures:\n",
    "    # save input(s)\n",
    "    fig = plt.figure(figsize=(15, 10), layout='constrained', dpi=resolution)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{filename.replace(\"tar/\", \"\")}_mp3d_rgb_img.png', bbox_inches='tight', format='png')\n",
    "\n",
    "    if config.backbone != 'mit_b2' and 'depth-1K' in config.modality_x:\n",
    "        fig = plt.figure(figsize=(15, 10), layout='constrained', dpi=resolution)\n",
    "        plt.imshow(dep, vmin=0, vmax=10, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'{filename.replace(\"tar/\", \"\")}_mp3d_dep_img.png', bbox_inches='tight', format='png')\n",
    "\n",
    "    if config.backbone != 'mit_b2' and 'normal-1K' in config.modality_x:\n",
    "        fig = plt.figure(figsize=(15, 10), layout='constrained', dpi=resolution)\n",
    "        plt.imshow(nor)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'{filename.replace(\"tar/\", \"\")}_mp3d_nor_img.png', bbox_inches='tight', format='png')\n",
    "\n",
    "    # save gt\n",
    "    fig = plt.figure(figsize=(15, 10), layout='constrained', dpi=resolution)\n",
    "    plt.imshow(seg_colors[gt])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{filename.replace(\"tar/\", \"\")}_mp3d_gt_img.png', bbox_inches='tight', format='png')\n",
    "    \n",
    "    # save prediction\n",
    "    fig = plt.figure(figsize=(15, 10), layout='constrained', dpi=resolution)\n",
    "    plt.imshow(seg_colors[pred])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{filename.replace(\"tar/\", \"\")}_mp3d_pred{miou}_img.png', bbox_inches='tight', format='png')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdb251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
