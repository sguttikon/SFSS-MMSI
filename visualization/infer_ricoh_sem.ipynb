{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmseg.core.evaluation import mean_iou\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from contextlib import redirect_stdout\n",
    "from easydict import EasyDict\n",
    "from models.builder import EncoderDecoder\n",
    "from utils.pyt_utils import load_model\n",
    "from utils.transforms import normalize\n",
    "from eval import get_ricoh3d_pan_pipeline as get_eval_pipeline\n",
    "from dataloader.dataloader import get_ricoh3d_pan_pipeline as get_train_pipeline\n",
    "\n",
    "trap = io.StringIO()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b83bc",
   "metadata": {},
   "source": [
    "# Ricoh3D RGB-D Panoramic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a921a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "config = EasyDict()\n",
    "\n",
    "config.root = '/home/guttikonda/Documents/OriginalWorks/Personal/SFSS-MMSI' # TODO: change this to your own path\n",
    "config.dataset_path = osp.join(config.root, 'datasets', 'Ricoh3D-1K')\n",
    "config.dataset_name = 'Stanford2D3DS' # TODO: change this to Stanford2D3DS or Structured3D\n",
    "config.ignore_index = 255\n",
    "config.image_height = 512\n",
    "config.image_width = 512\n",
    "config.norm_mean = np.array([0.485, 0.456, 0.406])\n",
    "config.norm_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "config.backbone = 'mit_b2' # TODO: change to 'mit_b2', 'dual_mit_b2', 'trio_mit_b2'\n",
    "config.pretrained_model = osp.join(config.root, 'pretrained', 'segformers/mit_b2.pth')\n",
    "config.decoder = 'DMLPDecoderV2'\n",
    "config.decoder_embed_dim = 512\n",
    "config.optimizer = 'AdamW'\n",
    "config.use_dcns = [True, False, False, False]\n",
    "\n",
    "config.batch_size = 1\n",
    "config.rgb = 'camera-rgb-1K'\n",
    "config.ann = 'camera-semantic-1K'\n",
    "config.modality_x = ['camera-depth-1K'] # TODO: change to 'camera-depth-1K', 'camera-normal-1K', 'camera-hha-1K'\n",
    "config.train_source = osp.join(config.dataset_path, 'train.txt')\n",
    "config.eval_source = osp.join(config.dataset_path, 'validation.txt')\n",
    "config.test_source = osp.join(config.dataset_path, 'test.txt')\n",
    "config.train_scale_array = [0.5, 0.75, 1, 1.25, 1.5, 1.75]\n",
    "if config.dataset_name == 'Stanford2D3DS':\n",
    "    config.fold = 'F1' # TODO: change this to F1, F2 or F3\n",
    "    config.num_classes = 13\n",
    "    pallete_path = 'assets/stanford2d3dpallete.npy'\n",
    "    name2label_path = 'assets/2d3ds_name2label.json'\n",
    "    if config.backbone == 'mit_b2':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_{config.fold}'))\n",
    "    elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-depth-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_Depth_{config.fold}'))\n",
    "    elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-normal-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_Normal_{config.fold}'))\n",
    "    elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-hha-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_HHA_{config.fold}'))\n",
    "    elif config.backbone == 'trio_mit_b2' and config.modality_x[0] == 'camera-depth-1K' and config.modality_x[1] == 'camera-normal-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Stanford2D3DS_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + f'_DMLPDecoderV2_Depth_Normal_{config.fold}'))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "elif config.dataset_name == 'Structured3D':\n",
    "    config.area = 'full'\n",
    "    config.num_classes = 40\n",
    "    pallete_path = 'assets/structured3dpallete.npy'\n",
    "    name2label_path = 'assets/structured3d_name2label.json'\n",
    "    if config.backbone == 'mit_b2':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Structured3D_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2'))\n",
    "    elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-depth-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Structured3D_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Depth'))\n",
    "    elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-normal-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Structured3D_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Normal'))\n",
    "    elif config.backbone == 'trio_mit_b2' and config.modality_x[0] == 'camera-depth-1K' and config.modality_x[1] == 'camera-normal-1K':\n",
    "        config.log_dir = os.path.abspath(osp.join(config.root, 'workdirs', 'Structured3D_1024x512',\n",
    "                                                  'log_' + config.dataset_name + '_' + config.backbone + '_DMLPDecoderV2_Depth_Normal'))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "config.checkpoint_pth = os.path.join(os.path.abspath(os.path.join(config.log_dir, 'checkpoint')), 'epoch-best.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eval_image_rgbX(image, modal_x1, modal_x2, norm_mean, norm_std):\n",
    "    image = normalize(image, norm_mean, norm_std)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = np.ascontiguousarray(image[None, :, :, :], dtype=np.float32)\n",
    "    image = torch.FloatTensor(image).cuda(device)\n",
    "\n",
    "    modal_x1 = normalize(modal_x1, norm_mean, norm_std)\n",
    "    modal_x1 = modal_x1.transpose(2, 0, 1)\n",
    "    modal_x1 = np.ascontiguousarray(modal_x1[None, :, :, :], dtype=np.float32)\n",
    "    modal_x1 = torch.FloatTensor(modal_x1).cuda(device)\n",
    "\n",
    "    modal_x2 = normalize(modal_x2, norm_mean, norm_std)\n",
    "    modal_x2 = modal_x2.transpose(2, 0, 1)\n",
    "    modal_x2 = np.ascontiguousarray(modal_x2[None, :, :, :], dtype=np.float32)\n",
    "    modal_x2 = torch.FloatTensor(modal_x2).cuda(device)\n",
    "    \n",
    "    return image, modal_x1, modal_x2\n",
    "\n",
    "def process_train_image_rgbX(image, modal_x1, modal_x2, norm_mean, norm_std):\n",
    "    image = np.ascontiguousarray(image[None, :, :, :], dtype=np.float32)\n",
    "    image = torch.FloatTensor(image).cuda(device)\n",
    "\n",
    "    modal_x1 = np.ascontiguousarray(modal_x1[None, :, :, :], dtype=np.float32)\n",
    "    modal_x1 = torch.FloatTensor(modal_x1).cuda(device)\n",
    "\n",
    "    modal_x2 = np.ascontiguousarray(modal_x2[None, :, :, :], dtype=np.float32)\n",
    "    modal_x2 = torch.FloatTensor(modal_x2).cuda(device)\n",
    "    \n",
    "    return image, modal_x1, modal_x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ricoh3d data\n",
    "valid_pipeline = get_eval_pipeline(config, split_name='test', mapping_name=config.dataset_name)\n",
    "process_image_rgbX = process_eval_image_rgbX\n",
    "config.eval_crop_size = [512, 1024]  # [height weight]\n",
    "\n",
    "# valid_pipeline = get_train_pipeline(config)\n",
    "# process_image_rgbX = process_train_image_rgbX\n",
    "# config.eval_crop_size = [512, 512]  # [height weight]\n",
    "\n",
    "valid_data_itr = iter(valid_pipeline)\n",
    "\n",
    "valid_labels = np.arange(config.num_classes).tolist() + [config.ignore_index]\n",
    "with open(os.path.join(config.dataset_path, pallete_path), 'rb') as f:\n",
    "    seg_colors = np.load(f)\n",
    "with open(os.path.join(config.dataset_path, name2label_path), 'r') as f:\n",
    "    name2id = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32511be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "for _ in range(2):\n",
    "    batch = next(valid_data_itr)\n",
    "image = batch[config.rgb]\n",
    "target = batch[config.ann]\n",
    "modal_x1 = batch[config.modality_x[0]]\n",
    "modal_x2 = batch[config.modality_x[1]] if len(config.modality_x) == 2 else modal_x1\n",
    "filename = batch['sample_token']\n",
    "\n",
    "image, modal_x1, modal_x2 = process_image_rgbX(image, modal_x1, modal_x2, config.norm_mean, config.norm_std)\n",
    "target = torch.LongTensor(target.astype('int32')).unsqueeze(0).cuda(device)\n",
    "assert set(torch.unique(target).tolist()).issubset(valid_labels), 'Unknown target label'\n",
    "\n",
    "# visualize input\n",
    "if config.backbone == 'mit_b2':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['GT']], figsize=(15, 12), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-depth-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth'], ['GT']], figsize=(15, 18), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-normal-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['NOR'], ['GT']], figsize=(15, 18), layout='constrained')\n",
    "elif config.backbone == 'dual_mit_b2' and config.modality_x[0] == 'camera-hha-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['HHA'], ['GT']], figsize=(15, 18), layout='constrained')\n",
    "elif config.backbone == 'trio_mit_b2' and config.modality_x[0] == 'camera-depth-1K' and config.modality_x[1] == 'camera-normal-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth'], ['NOR'], ['GT']], figsize=(15, 24), layout='constrained')\n",
    "elif config.backbone == 'trio_mit_b2' and config.modality_x[0] == 'camera-depth-1K' and config.modality_x[1] == 'camera-hha-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['Depth'], ['HHA'], ['GT']], figsize=(15, 24), layout='constrained')\n",
    "elif config.backbone == 'trio_mit_b2' and config.modality_x[0] == 'camera-normal-1K' and config.modality_x[1] == 'camera-hha-1K':\n",
    "    fig, axs = plt.subplot_mosaic(\n",
    "        [['RGB'], ['NOR'], ['HHA'], ['GT']], figsize=(15, 24), layout='constrained')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "img = image.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "img = (img * 255.0).astype('uint8')\n",
    "plt_img1 = axs['RGB'].imshow(img)\n",
    "axs['RGB'].set_axis_off()\n",
    "axs['RGB'].set_title('RGB Input')\n",
    "\n",
    "if config.backbone != 'mit_b2' and 'camera-depth-1K' in config.modality_x:\n",
    "    dep = modal_x1.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    # dep = (dep * 255.0).astype('uint8')\n",
    "    # dep = np.where(dep[..., 0] == 255, 0.0, dep[..., 0] / 255.0) * 128.0\n",
    "    dep = np.where(dep[..., 0] == 1.0, 0.0, dep[..., 0]) * 128.0\n",
    "    plt_img2 = axs['Depth'].imshow(dep, vmin=0, vmax=10, cmap='jet')\n",
    "    axs['Depth'].set_axis_off()\n",
    "    axs['Depth'].set_title('Depth Input')\n",
    "\n",
    "if config.backbone != 'mit_b2' and 'camera-hha-1K' in config.modality_x:\n",
    "    hha = modal_x1 if config.modality_x[0] == 'camera-hha-1K' else modal_x2\n",
    "    hha = hha.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    hha = (hha * 255.0).astype('uint8')\n",
    "    plt_img3 = axs['HHA'].imshow(hha)\n",
    "    axs['HHA'].set_axis_off()\n",
    "    axs['HHA'].set_title('HHA Input')\n",
    "    \n",
    "if config.backbone != 'mit_b2' and 'camera-normal-1K' in config.modality_x:\n",
    "    nor = modal_x1 if config.modality_x[0] == 'camera-normal-1K' else modal_x2\n",
    "    nor = nor.squeeze().permute(1, 2, 0).cpu().numpy() * config.norm_std + config.norm_mean\n",
    "    nor = (nor * 255.0).astype('uint8')\n",
    "    plt_img3 = axs['NOR'].imshow(nor)\n",
    "    axs['NOR'].set_axis_off()\n",
    "    axs['NOR'].set_title('Normal Input')\n",
    "\n",
    "groundtruth = target.long() + 1\n",
    "gt = groundtruth.squeeze().cpu().numpy().astype('uint8')\n",
    "axs['GT'].imshow(seg_colors[gt])\n",
    "axs['GT'].set_axis_off()\n",
    "axs['GT'].set_title('Semantic GT')\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color=seg_colors[seg_val]/255., label=seg_lbl)\n",
    "    for seg_lbl, seg_val in name2id.items()\n",
    "]\n",
    "\n",
    "plt.legend(handles=patches, loc='lower center', ncol=7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "network = EncoderDecoder(cfg=config, criterion=None, norm_layer=nn.BatchNorm2d)\n",
    "model = load_model(network, config.checkpoint_pth).to(device)\n",
    "\n",
    "# redirect stdout\n",
    "with redirect_stdout(trap):\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "assert list(image.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x1.shape[-2:]) == config.eval_crop_size\n",
    "assert list(modal_x2.shape[-2:]) == config.eval_crop_size\n",
    "with torch.no_grad():\n",
    "    if config.backbone == 'mit_b2' or len(config.modality_x) == 1:\n",
    "        score = model.forward(image, modal_x1)\n",
    "    elif len(config.modality_x) == 2:\n",
    "        score = model.forward(image, modal_x1, modal_x2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "output = torch.exp(score)\n",
    "iou_result = mean_iou(results=output.argmax(1).cpu().numpy(), gt_seg_maps=target.cpu().numpy(),\n",
    "                      num_classes=config.num_classes, ignore_index=config.ignore_index, nan_to_num=None,\n",
    "                      label_map=dict(), reduce_zero_label=False)\n",
    "\n",
    "id2class = ['beam', 'board', 'bookcase', 'ceiling', 'chair', 'clutter', 'column',\n",
    "            'door', 'floor', 'sofa', 'table', 'wall', 'window']\n",
    "for name, iou, acc in zip(id2class, iou_result['IoU'], iou_result['Acc']):\n",
    "    print(f'{name:20s}:    iou {iou*100:5.3f}    /    acc {acc*100:5.3f}')\n",
    "\n",
    "print('Eval mAcc: {:.3f}, aAcc: {:.3f}, mIoU: {:.3f}'.format(np.nanmean(iou_result['Acc']) * 100,\n",
    "                                                             iou_result['aAcc'] * 100,\n",
    "                                                             np.nanmean(iou_result['IoU']) * 100))\n",
    "miou = round(np.nanmean(iou_result['IoU']) * 100, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52109e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    [['GT'], ['Pred']], figsize=(15, 12), layout='constrained')\n",
    "\n",
    "axs['GT'].imshow(seg_colors[gt])\n",
    "axs['GT'].set_axis_off()\n",
    "axs['GT'].set_title('Semantic GT')\n",
    "\n",
    "predict = torch.argmax(output.long(), dim=1) + 1\n",
    "pred = predict.squeeze().cpu().numpy().astype('uint8')\n",
    "unlabeled = np.array(config.ignore_index + 1).astype(np.uint8)\n",
    "if config.backbone == 'mit_b2':\n",
    "    pred[img.sum(-1) == 0] = unlabeled  # mask as unknown id: 0\n",
    "# pred[gt == unlabeled] = unlabeled  # mask as unknown id\n",
    "axs['Pred'].imshow(seg_colors[pred])\n",
    "axs['Pred'].set_axis_off()\n",
    "axs['Pred'].set_title('Semantic Prediction')\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color=seg_colors[seg_val]/255., label=seg_lbl)\n",
    "    for seg_lbl, seg_val in name2id.items()\n",
    "]\n",
    "\n",
    "plt.legend(handles=patches, loc='lower center', ncol=7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476ba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
